{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson5 画像からキャプションを生成してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "本Lessonで学んだことを生かして，画像からのキャプション生成を実装してみましょう。\n",
    "\n",
    "ネットワークの形などは特に制限を設けませんし、今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません。\n",
    "\n",
    "精度上位者はリーダーボードに掲載させていただきます。（精度の評価はBLEUスコアによって行います。）\n",
    "\n",
    "## 目標値\n",
    "\n",
    "BLUE: 0.14\n",
    "\n",
    "## ルール\n",
    "\n",
    "- Lesson中で登場したbeam_searchを使って以下のサンプルも参考にしながらキャプションを生成しcsvファイルに出力して下さい。\n",
    "- **BLEUスコア(4-gramまで)**で評価します。\n",
    "\n",
    "## 評価について\n",
    "\n",
    "- テストデータ(x_test)に対する予測ラベルをcsvファイルで提出してください。\n",
    "- ファイル名はsubmission.csvとしてください。\n",
    "- 予測ラベルのy_testに対する精度 (F値)で評価します。\n",
    "- 毎日24時にテストデータの一部に対する精度でLeader Boardを更新します。\n",
    "- 最終的な評価はテストデータ全体に対する精度でおこないます。\n",
    "\n",
    "## サンプルコード\n",
    "\n",
    "**次のセルで指定されているx_train, y_trainのみを使って学習させてください。　**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sort_data_by_length(data_x, data_y):\n",
    "    data_x_lens = [len(datum) for datum in data_x]\n",
    "    sorted_data_indexes = sorted(range(len(data_x_lens)), key=lambda x: -data_x_lens[x])\n",
    "    \n",
    "    data_x = [data_x[i] for i in sorted_data_indexes]\n",
    "    data_y = [data_y[i] for i in sorted_data_indexes]\n",
    "    \n",
    "    return data_x, data_y\n",
    "\n",
    "def load_data():\n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/lesson5/data/x_train.npy')\n",
    "    y_train = np.load('/root/userspace/public/lesson5/data/y_train.npy')\n",
    "    i2w = np.load('/root/userspace/public/lesson5/data/i2w.npy').item()\n",
    "    w2i = np.load('/root/userspace/public/lesson5/data/w2i.npy').item()\n",
    "    vocab_size = len(w2i)\n",
    "\n",
    "    \n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/lesson5/data/x_test.npy')\n",
    "    \n",
    "    y_train, x_train = sort_data_by_length(y_train, x_train)\n",
    "\n",
    "    return (x_train, y_train, i2w, w2i, vocab_size, x_test)\n",
    "\n",
    "x_train, y_train, i2w, w2i, vocab_size, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 197s 698ms/step - loss: 56.8465\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 50.9095\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 46.6106\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 44.6208\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 43.3676\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 42.4177\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 41.3153\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 40.6286\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 180s 638ms/step - loss: 39.2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:30<00:00, 11.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Activation, Flatten, Reshape, dot, Permute, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "sys.path.append('/root/userspace/public/lesson5/master')\n",
    "\n",
    "from utils import Node\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "batch_size = 32\n",
    "emb_dim = 32\n",
    "hid_dim = 128\n",
    "\n",
    "### Encoder ###\n",
    "x = Input(shape=(224, 224, 3))\n",
    "x_normalized = Lambda(lambda x: x / 255.)(x) # [0, 255) -> [0, 1)\n",
    "encoder = VGG16(weights='imagenet', include_top=False, input_tensor=x_normalized)\n",
    "\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "u = Flatten()(encoder.output)\n",
    "\n",
    "### Decoder ###\n",
    "u_map = Reshape((7*7, 512))(u)\n",
    "\n",
    "h_0 = Dense(hid_dim)(u)\n",
    "cell_0 = Dense(hid_dim)(u)\n",
    "\n",
    "y = Input(shape=(None,), dtype='int32')\n",
    "y_in = Lambda(lambda x: x[:, :-1])(y)\n",
    "y_out = Lambda(lambda x: x[:, 1:])(y)\n",
    "\n",
    "mask = Lambda(lambda x: K.cast(K.not_equal(x, w2i['<pad>']), 'float32'))(y_out)\n",
    "\n",
    "embedding = Embedding(vocab_size, emb_dim)\n",
    "lstm = LSTM(hid_dim, activation='tanh', return_sequences=True, return_state=True)\n",
    "\n",
    "y_emb = embedding(y_in)\n",
    "h, _, _ = lstm(y_emb, initial_state=[h_0, cell_0])\n",
    "\n",
    "### Attention ###\n",
    "dense_att = Dense(hid_dim)\n",
    "_u_map = dense_att(u_map)\n",
    "score = dot([_u_map, h], axes=-1)\n",
    "\n",
    "permute_att1 = Permute((2, 1))\n",
    "activation_att = Activation('softmax')\n",
    "score = permute_att1(score)\n",
    "a = activation_att(score)\n",
    "\n",
    "permute_att2 = Permute((2, 1))\n",
    "context = dot([u_map, a], axes=(1, 2))\n",
    "context = permute_att2(context)\n",
    "\n",
    "dense_output1 = Dense(hid_dim)\n",
    "dense_output2 = Dense(vocab_size)\n",
    "softmax = Activation('softmax')\n",
    "h_tilde = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=2))([h, context])\n",
    "h_tilde = dense_output1(h_tilde)\n",
    "y_pred = dense_output2(h_tilde)\n",
    "y_pred = softmax(y_pred)\n",
    "\n",
    "### Learning ###\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -K.mean(K.sum(K.sum(y_true * K.log(K.clip(y_pred, 1e-10, 1)), axis=-1) * mask, axis=1))\n",
    "\n",
    "model = Model([x, y], y_pred)\n",
    "\n",
    "model.compile(loss=cross_entropy, optimizer='rmsprop')\n",
    "\n",
    "def generator(data_X, data_y, batch_size=32):\n",
    "\n",
    "    n_batches = math.ceil(len(data_X) / batch_size)\n",
    "\n",
    "    while True:\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "\n",
    "            data_x_mb = data_X[start:end]\n",
    "            data_y_mb = data_y[start:end]\n",
    "\n",
    "            data_x_mb = np.array(data_x_mb).astype('float32') / 255.\n",
    "            data_y_mb = pad_sequences(data_y_mb, dtype='int32', padding='post', value=w2i['<pad>'])\n",
    "            data_y_mb_oh = np.array([np_utils.to_categorical(datum_y, vocab_size) for datum_y in data_y_mb[:, 1:]])\n",
    "\n",
    "            yield [data_x_mb, data_y_mb], data_y_mb_oh\n",
    "\n",
    "n_batches_train = math.ceil(len(x_train) / batch_size)\n",
    "\n",
    "try:\n",
    "    model.fit_generator(\n",
    "        generator(x_train, y_train),\n",
    "        epochs=10,\n",
    "        steps_per_epoch=n_batches_train,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "### Predict ###\n",
    "encoder_model = Model([x], [u_map, h_0, cell_0])\n",
    "\n",
    "u_map_inpt = Input(shape=(7*7, 512,))\n",
    "h_tm1 = Input(shape=(hid_dim,))\n",
    "cell_tm1 = Input(shape=(hid_dim,))\n",
    "y_t = Input(shape=(1,))\n",
    "y_emb_t = embedding(y_t)\n",
    "_, h_t, cell_t = lstm(y_emb_t, initial_state=[h_tm1, cell_tm1])\n",
    "h_t = Lambda(lambda x: x[:, None, :])(h_t)\n",
    "cell_t = Lambda(lambda x: x[:, None, :])(cell_t)\n",
    "\n",
    "_u_map = dense_att(u_map_inpt)\n",
    "score_t = dot([_u_map, h_t], axes=-1)\n",
    "\n",
    "score_t = permute_att1(score_t)\n",
    "a_t = activation_att(score_t)\n",
    "\n",
    "context_t = dot([u_map_inpt, a_t], axes=(1, 2))\n",
    "context_t = permute_att2(context_t)\n",
    "\n",
    "h_tilde_t = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=2))([h_t, context_t])\n",
    "h_tilde_t = dense_output1(h_tilde_t)\n",
    "pred_t = dense_output2(h_tilde_t)\n",
    "\n",
    "decoder_model = Model([y_t, h_tm1, cell_tm1, u_map_inpt], [pred_t, h_t, cell_t, a_t])\n",
    "\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(x, 1e-10, x))\n",
    "\n",
    "def beam_search(x, max_len=100, k=3):\n",
    "    u_map, h_tm1, cell_tm1 = encoder_model.predict(x)\n",
    "    y_tm1 = np.array([w2i['<s>']])\n",
    "\n",
    "    root = Node(w2i['<s>'])\n",
    "\n",
    "    # [score, y_tm1, h_tm1, cell_tm1, a, y_pred]\n",
    "    candidates = [[0, y_tm1, h_tm1, cell_tm1, [], y_tm1]]\n",
    "\n",
    "    t = 0\n",
    "    while t < max_len:\n",
    "        root.depth += 1\n",
    "        t += 1\n",
    "\n",
    "        # すべての候補を一時的に保管するリスト\n",
    "        tmp_candidates = []\n",
    "\n",
    "        # </s>がすべての候補で出力されたかどうかのフラッグ\n",
    "        end_flag = True\n",
    "        for score_tm1, y_tm1, h_tm1, cell_tm1, a, y_pred in candidates:\n",
    "            a = copy.deepcopy(a)\n",
    "            if y_tm1[0] == w2i['</s>']:\n",
    "                tmp_candidates.append([score_tm1, y_tm1, h_tm1, cell_tm1, a, y_pred])\n",
    "            else:\n",
    "                end_flag = False\n",
    "                y_t, h_t, cell_t, a_t = decoder_model.predict([y_tm1, h_tm1, cell_tm1, u_map])\n",
    "                h_t, cell_t = h_t[:, 0], cell_t[:, 0]\n",
    "                a.append(a_t.flatten())\n",
    "\n",
    "                # 対数化\n",
    "                y_t = np_log(y_t.flatten())\n",
    "\n",
    "                # 確率の高い単語とそのidを取得\n",
    "                y_t, s_t = np.argsort(y_t)[::-1][:k], np.sort(y_t)[::-1][:k]\n",
    "\n",
    "                # スコア (対数尤度) を蓄積\n",
    "                score_t = score_tm1 + s_t\n",
    "\n",
    "                # すべての候補を一時的に保管\n",
    "                tmp_candidates.extend(\n",
    "                    [[score_tk, np.array([y_tk]), h_t, cell_t, a, np.append(y_pred, [y_tk])]\n",
    "                        for score_tk, y_tk in zip(score_t, y_t)]\n",
    "                )\n",
    "        if end_flag:\n",
    "            break\n",
    "\n",
    "        candidates = sorted(tmp_candidates, key=lambda x: -x[0]/len(x[-1]))[:k]\n",
    "\n",
    "    return candidates[0][-1]\n",
    "\n",
    "pred_list = []\n",
    "for i in tqdm(range(x_test.shape[0])):\n",
    "    pred_y = beam_search(x_test[i:i+1], k=3, max_len=20)\n",
    "    pred_list.append(list(pred_y))\n",
    "\n",
    "with open('/root/userspace/lesson5/blank/submission.csv', 'w') as file:\n",
    "    writer = csv.writer(file, lineterminator='\\n')\n",
    "    writer.writerows(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
