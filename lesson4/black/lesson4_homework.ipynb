{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson4 ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "RNNを用いて高精度な英日翻訳器を実装してみましょう。\n",
    "\n",
    "ネットワークの形などは特に制限を設けませんし、今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません。\n",
    "\n",
    "精度上位者はリーダーボードに掲載させていただきます。（精度の評価はBLEUスコアによって行います。）\n",
    "\n",
    "## 目標値\n",
    "\n",
    "BLEU: 0.15\n",
    "\n",
    "## ルール\n",
    "\n",
    "- 以下のサンプルも参考にしながら翻訳文を生成しcsvファイルに出力して下さい。\n",
    "- BLEUスコア(4-gramまで)で評価します。\n",
    "- 学習データとテストデータの入力の系列長はpaddingで揃えてあります。\n",
    "\n",
    "## 評価について\n",
    "\n",
    "- テストデータ(x_test)に対する予測ラベルをcsvファイルで提出してください。\n",
    "- ファイル名はsubmission.csvとしてください。\n",
    "- 予測ラベルのy_testに対する精度 (BLEU)で評価します。\n",
    "- 毎日24時にテストデータの一部に対する精度でLeader Boardを更新します。\n",
    "- 最終的な評価はテストデータ全体に対する精度でおこないます。\n",
    "\n",
    "## サンプルコード\n",
    "\n",
    "次のセルで指定されているx_train, y_trainのみを使って学習させてください。　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data():\n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/lesson4/data/x_train.npy')\n",
    "    y_train = np.load('/root/userspace/public/lesson4/data/y_train.npy')\n",
    "    tokenizer_en = np.load('/root/userspace/public/lesson4/data/tokenizer_en.npy').item()\n",
    "    tokenizer_ja = np.load('/root/userspace/public/lesson4/data/tokenizer_ja.npy').item()\n",
    " \n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/lesson4/data/x_test.npy')\n",
    "\n",
    "    return (x_train, y_train, tokenizer_en, tokenizer_ja, x_test)\n",
    "\n",
    "x_train, y_train, tokenizer_en, tokenizer_ja, x_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, LSTM\n",
    "import csv\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "\n",
    "en_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
    "\n",
    "seqX_len = len(x_train[0])\n",
    "seqY_len = len(y_train[0])\n",
    "\n",
    "encoder_inputs = Input(shape=(seqX_len,))\n",
    "encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs)\n",
    "_, *encoder_states = LSTM(hid_dim, return_state=True)(encoder_embedded)\n",
    "\n",
    "decoder_inputs = Input(shape=(seqY_len,))\n",
    "decoder_embedding = Embedding(ja_vocab_size, emb_dim)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_dense = Dense(ja_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=128, epochs=1, validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
    "decoder_inputs = Input(shape=(1,))\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(input_seq, bos_eos, max_output_length):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])\n",
    "    output_seq = bos_eos[0][:]\n",
    "\n",
    "    while True:\n",
    "        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "\n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "output = [decode_sequence(x_test[i][np.newaxis,:], bos_eos, 100)[1:-1] for i in range(len(x_test))]\n",
    "\n",
    "with open('/root/userspace/submission.csv', 'w') as file:\n",
    "    writer = csv.writer(file, lineterminator='\\n')\n",
    "    writer.writerows(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
